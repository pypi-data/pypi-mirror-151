Metadata-Version: 2.1
Name: upm-oct-dataset-utils
Version: 0.12.1
Summary: Dataset utility package for UPM OCT/OCTA study (MS, NMO and RIS)
Home-page: https://github.com/pgmesa-upm/upm_oct_dataset_utils
Author: Pablo GarcÃ­a Mesa
Author-email: pgmesa.sm@gmail.com
License: UNKNOWN
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3.9
Classifier: License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)
Classifier: Operating System :: OS Independent
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE.txt
Requires-Dist: absl-py (==0.15.0)
Requires-Dist: argon2-cffi (==21.3.0)
Requires-Dist: argon2-cffi-bindings (==21.2.0)
Requires-Dist: asttokens (==2.0.5)
Requires-Dist: astunparse (==1.6.3)
Requires-Dist: attrs (==21.4.0)
Requires-Dist: backcall (==0.2.0)
Requires-Dist: beautifulsoup4 (==4.11.1)
Requires-Dist: bleach (==5.0.0)
Requires-Dist: cachetools (==5.1.0)
Requires-Dist: certifi (==2021.10.8)
Requires-Dist: cffi (==1.15.0)
Requires-Dist: charset-normalizer (==2.0.11)
Requires-Dist: colorama (==0.4.4)
Requires-Dist: cycler (==0.11.0)
Requires-Dist: debugpy (==1.6.0)
Requires-Dist: decorator (==5.1.1)
Requires-Dist: defusedxml (==0.7.1)
Requires-Dist: distlib (==0.3.4)
Requires-Dist: entrypoints (==0.4)
Requires-Dist: executing (==0.8.3)
Requires-Dist: fastjsonschema (==2.15.3)
Requires-Dist: flatbuffers (==1.12)
Requires-Dist: fonttools (==4.29.1)
Requires-Dist: gast (==0.4.0)
Requires-Dist: google-auth (==2.6.6)
Requires-Dist: google-auth-oauthlib (==0.4.6)
Requires-Dist: google-pasta (==0.2.0)
Requires-Dist: grpcio (==1.34.1)
Requires-Dist: h5py (==3.1.0)
Requires-Dist: idna (==3.3)
Requires-Dist: imageio (==2.19.1)
Requires-Dist: importlib-metadata (==4.11.3)
Requires-Dist: ipykernel (==6.13.0)
Requires-Dist: ipython (==8.3.0)
Requires-Dist: ipython-genutils (==0.2.0)
Requires-Dist: jedi (==0.18.1)
Requires-Dist: Jinja2 (==3.1.2)
Requires-Dist: jsonschema (==4.5.1)
Requires-Dist: jupyter-client (==7.3.1)
Requires-Dist: jupyter-core (==4.10.0)
Requires-Dist: jupyterlab-pygments (==0.2.2)
Requires-Dist: keras (==2.8.0)
Requires-Dist: keras-nightly (==2.5.0.dev2021032900)
Requires-Dist: Keras-Preprocessing (==1.1.2)
Requires-Dist: kiwisolver (==1.3.2)
Requires-Dist: libclang (==14.0.1)
Requires-Dist: Markdown (==3.3.7)
Requires-Dist: MarkupSafe (==2.1.1)
Requires-Dist: matplotlib (==3.5.1)
Requires-Dist: matplotlib-inline (==0.1.3)
Requires-Dist: mistune (==0.8.4)
Requires-Dist: nbclient (==0.6.3)
Requires-Dist: nbconvert (==6.5.0)
Requires-Dist: nbformat (==5.4.0)
Requires-Dist: nest-asyncio (==1.5.5)
Requires-Dist: networkx (==2.8)
Requires-Dist: notebook (==6.4.11)
Requires-Dist: numpy (==1.22.4)
Requires-Dist: oauthlib (==3.2.0)
Requires-Dist: opencv-python (==4.5.5.64)
Requires-Dist: opt-einsum (==3.3.0)
Requires-Dist: packaging (==21.3)
Requires-Dist: pandas (==1.4.0)
Requires-Dist: pandas-read-xml (==0.3.1)
Requires-Dist: pandocfilters (==1.5.0)
Requires-Dist: parso (==0.8.3)
Requires-Dist: pickleshare (==0.7.5)
Requires-Dist: Pillow (==9.0.1)
Requires-Dist: prometheus-client (==0.14.1)
Requires-Dist: prompt-toolkit (==3.0.29)
Requires-Dist: protobuf (==3.20.1)
Requires-Dist: psutil (==5.9.0)
Requires-Dist: pure-eval (==0.2.2)
Requires-Dist: pyarrow (==7.0.0)
Requires-Dist: pyasn1 (==0.4.8)
Requires-Dist: pyasn1-modules (==0.2.8)
Requires-Dist: pycparser (==2.21)
Requires-Dist: Pygments (==2.12.0)
Requires-Dist: pyparsing (==3.0.7)
Requires-Dist: pyrsistent (==0.18.1)
Requires-Dist: python-dateutil (==2.8.2)
Requires-Dist: pytz (==2021.3)
Requires-Dist: PyWavelets (==1.3.0)
Requires-Dist: pywin32 (==304)
Requires-Dist: pywinpty (==2.0.5)
Requires-Dist: PyYAML (==6.0)
Requires-Dist: pyzmq (==22.3.0)
Requires-Dist: requests (==2.27.1)
Requires-Dist: requests-oauthlib (==1.3.1)
Requires-Dist: rsa (==4.8)
Requires-Dist: scikit-image (==0.19.2)
Requires-Dist: scipy (==1.8.0)
Requires-Dist: Send2Trash (==1.8.0)
Requires-Dist: six (==1.15.0)
Requires-Dist: soupsieve (==2.3.2.post1)
Requires-Dist: stack-data (==0.2.0)
Requires-Dist: tensorboard (==2.8.0)
Requires-Dist: tensorboard-data-server (==0.6.1)
Requires-Dist: tensorboard-plugin-wit (==1.8.1)
Requires-Dist: tensorflow (==2.8.0)
Requires-Dist: tensorflow-estimator (==2.5.0)
Requires-Dist: tensorflow-io-gcs-filesystem (==0.26.0)
Requires-Dist: termcolor (==1.1.0)
Requires-Dist: terminado (==0.13.3)
Requires-Dist: tf-estimator-nightly (==2.8.0.dev2021122109)
Requires-Dist: tifffile (==2022.5.4)
Requires-Dist: tinycss2 (==1.1.1)
Requires-Dist: tornado (==6.1)
Requires-Dist: tqdm (==4.64.0)
Requires-Dist: traitlets (==5.2.0)
Requires-Dist: typing-extensions (==3.7.4.3)
Requires-Dist: urllib3 (==1.26.8)
Requires-Dist: wcwidth (==0.2.5)
Requires-Dist: webencodings (==0.5.1)
Requires-Dist: Werkzeug (==2.1.2)
Requires-Dist: wrapt (==1.12.1)
Requires-Dist: xmltodict (==0.12.0)
Requires-Dist: zipfile36 (==0.1.3)
Requires-Dist: zipp (==3.8.0)

# UPM package for managing the OCT/OCTA study dataset

Package that offers some functionalities to easily work, create, manage and organize a dataset for an study that works with raw (.img) OCT/OCTA volumes and XML scans analysis, exported from the Cirrus Zeiss 5000 with the Zeiss research license. This code in specific for the UPM multiple sclerosis, NMO and RIS study (2021-2022), but it can serve as a base to fit a great range of other necessities related with the topic. The idea is to have two different directories to store data: a raw dataset with the exported data (.img) from the device and a clean dataset where the processed data from the raw dataset will be stored. To train an AI with the clean dataset, is as simple as clean_ds.get_data_paths(-query-) to get all data paths you need and then create for example a tensorflow dataset to load the data durig the training.

Each study is defined to be:
- 4 OCT, 2 macular volumes (one for each eye), 2 optic-disc volumes (one for each eye)
- 4 OCTA 2 macular volumes (one for each eye), 2 optic-disc volumes (one for each eye)
- 2 retinographies (one for each eye)
- x number of XML files, that expect to contain the analysis of each OCT and OCTA scan (they can be in different files).

If more than one scan is added to the OCT and OCTA scans, the most recent one will be used.

In OCT the important file is the 'cube_z.img' and in OCTA 'flowcube_z.img'

## Installation
```
pip install upm_oct_dataset_utils
```
## Modules 

### dataset_classes
Classes that represent a layer of abstraction to easily query the file system tree of the dataset where the images are stored (hard disk, computer ...)
Default arquitecure that the tree directory must follow in the raw dataset:
```
- dataset_path
    (groups)
    - control
        (patients)
        - patient-1
            (exported with Zeiss research licence)
            - PCZMI515190478 20160414
                PCZMI515190478_Macular Cube 512x128_4-14-2016_9-5-35_OD_sn99960_cube_z.img
                PCZMI515190478_Macular Cube 512x128_4-14-2016_9-5-35_OD_sn99960_cube_raw.img
                ...
                - retinography
                    - O(S/D)_adqu-date_retinography.jpg
            - PCZMI515190478 20170517
                ...
            CZMI... .xml
        - patient-2
            ...
        - ...
    - MS
        ...
    - NMO
        ...
    - RIS
        ...
```

### oct_processing_lib
To process, read and easily work with the raw (.img) images from Cirrus Zeiss 5000. It also offers some functions to manage and reconstruct OCTA volumes.
The file system tree of the dataset should be as follows 
```
# Reconstruct OCTA from volume and get 2D projected numpy array
data = process_cube(
    data_path, 'OCTA', 'macula', 
    resize=(int(width_scale_factor*1024), 1024) if resize else None
).rotate_face(axe='x').resize_slices((350,350)).project().as_nparray()
```

### visualization_lib
To visualize OCTA reconstructions and OCT/OCTA volumes with the possibility to animate the volume as a short movie.
The functions admits arrays of images to show and arrays of different volumes to animate at the same time (when multi option is set to True)
```
def show_image(image, title:list[str]=None, subplot_size:tuple[int,int]=None, 
                    cmap:str='jet', colorbar:bool=None, multi:bool=False, show:bool=True):
    ...

def animate_volume(volume, figure=None, title:list[str]=None, subplot_size:tuple[int,int]=None, 
                    cmap:str='jet', colorbar:bool=None, multi:bool=False, t_milisec:int=4000, repeat=True):
    ...
```

### xml_processing_lib
To process and read 1 or more XML analysis from the Cirrus Zeiss 5000
```
# Remove trash oct info and returns a clean XML with the useful data (removes "TRACKINGDETAILS" field and other minor stuff)
processed_xml:dict = process_xmlscans(xml_path, study_date, xml_scan_names_to_process)
```

## Usage Example
Create a query to get all optic-disk OCTA images of left eye of the control group patients from 1 to 7 
```
raw_dataset = RawDataset(raw_dataset_path)
raw_data_paths:dict = raw_dataset.get_data_paths(
    group='control', patient_num=[1,2,3,4,5,6,7], data_type='OCTA', zone="optic-disc", eye="left"
)
```
See the state of the datasets, the adquisitions that are missing from each patient study or are pending to be exported or the studies and adquisitions that have not been processed yet (raw_dataset.show_info(-query-) or clean_dataset.show_info(-query-))
```
+ RAW DATASET INFO (Path -> 'D:\study_datasets\raw_dataset')

        - Adquisitions per patient study:
            -> 4 OCT (macular_OD, macular_OS, optic-nerve_OD, optic-nerve_OS)
            -> 4 OCTA (macular_OD, macular_OS, optic-nerve_OD, optic-nerve_OS)
            -> 2 retinographies (OD, OS)
            -> 8 scans XML analysis report

----------------------------------------------------
+ CONTROL GROUP (size=21)
- 'patient-1' (studies=1) has all adquisitions
- 'patient-2' (studies=1) has all adquisitions
- 'patient-3' (studies=1) has all adquisitions
- 'patient-4' (studies=1) has all adquisitions
...
----------------------------------------------------
----------------------------------------------------
+ MS GROUP (size=26)
- 'patient-1' (studies=1) has all adquisitions
- 'patient-2' (studies=1) has all adquisitions
- 'patient-6' (studies=1) has all adquisitions
- 'patient-7' (studies=1) has missing info:
    {
        "PCZMI515190478 20160414": {
            "OCTA": "macula left missing",
            "XML": {
                "OCTA_macula_OS": "missing"
            }
        }
    }
...
+ SUMMARY (queried-studies=26):
     -> OCT Cubes => 76/104 (73.08%) -> (28 missing)
     -> OCTA Cubes => 51/104 (49.04%) -> (53 missing)
     -> Retina Images => 24/52 (46.15%) -> (28 missing)
     -> XML scans => 127/208 (61.06%) -> (81 missing)
 -> Global data = 278/468 (59.4%) -> (190 missing)
----------------------------------------------------
----------------------------------------------------
 + NMO GROUP (size=1)
 - 'patient-1' (studies=1) has all adquisitions
 + SUMMARY:
     -> OCT Cubes => 4/4 (100.0%) -> (0 missing)
     -> OCTA Cubes => 4/4 (100.0%) -> (0 missing)
     -> Retina Images => 2/2 (100.0%) -> (0 missing)
     -> XML scans => 8/8 (100.0%) -> (0 missing)
 -> Global data = 18/18 (100.0%) -> (0 missing)
----------------------------------------------------
----------------------------------------------------
 + RIS GROUP (size=0)
     -> This group is empty
----------------------------------------------------
```
Process all raw images exported with Zeiss research License and save them in standard format in the clean dataset
```
clean_dataset = CleanDataset(clean_dataset_path)
raw_data_paths:dict = raw_dataset.get_data_paths()
for group, patients_data in raw_data_paths.items():
    for patient, study_data in patients_data.items():
        clean_dataset.create_patient(grp, patient_num=p_num)
        for study, dtypes in study_data.items():
            clean_dataset.create_study(group, patient_num, study)
            for data_type, zones_data in dtypes.items():
                clean_path = clean_dataset.get_dir_path(group=grp, patient_num=p_num, study=study, data_type=dtype)
                for zone, eye_data in zonde_data.items():
                    for eye, data_path in eye_data.items():
                        data = process_cube(
                            data_path, data_type, zone, 
                            resize=(int(width_scale_factor*1024), 1024) if resize else None
                        ).rotate_face(axe='x').resize_slices((350,350)).project().as_nparray()
                        data_path = Path(data_path)
                        raw_file_info = raw_dataset.split_file_name(data_path.name, dtype)
                        adq_date = raw_file_info['adquisition_date']
                        adq_name = zone_info['adquisitions_name']
                        file_name = patient+"_"+adq_name[data_type]+"_"+adq_date+"_"+eye_conv+'.tif'
                        file_path = clean_path/file_name
                        tiff.imwrite(file_path, data)
```
Clean Dataset directory tree
```
- dataset_path
    (groups)
    - control
        (patients)
        - patient-1
            - study_20-11-2021
                - OCT
                    - patient-1_adqu-type_adqu-date_O(S/D).tiff
                - OCTA
                    ...
                - retinography
                    - patient-1_retinography_adqu-date_O(S/D).jpg
                - patient-1_analysis.json
            - study_23-1-2022
                ...
        - patient-2
            ...
        - ...
    - MS
        ...
    - NMO
        ...
    - RIS
        ...
```


